{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1. Regular Expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables of regular expression\n",
    "\n",
    "### pattern       matches        example\n",
    "     \\w+          word           'Magic'\n",
    "     \\d           digit           9\n",
    "     \\s           space           ''\n",
    "     \\S           no space        'no_space'\n",
    "     .*           wildcard        'username74'\n",
    "     + or *        greedy match    'aaaaa'\n",
    "     [a-z]         lowercase        'abced'\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About re.match and re.search\n",
    "\n",
    "re.match: match from the beignning of the sentence;\n",
    "\n",
    "re.search: look for the whole sentence and try to match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition: Turning a string into smaller chunks (tokens)\n",
    "\n",
    "#### Usually use nltk library\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'there', '!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "a = word_tokenize('Hi there!')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_endings = r\"[.?!]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#nlp', '#python']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize,TweetTokenizer\n",
    "tweets = ['This is the best #nlp exercise ive found online! #python']\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "hashtags = regexp_tokenize(tweets[0],pattern1) #Note: the order is reverse from the re.math or re.search or re.findall\n",
    "print(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer(tweets)\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 2. Word Counts with bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words:\n",
    "\n",
    "1. Basic method of finding topics in a text\n",
    "\n",
    "2. Need to first create tokens using tokenization\n",
    "\n",
    "3. then count up all the tokens -> the more frequent a word, the more important it might be\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'The': 3,\n",
       "         'cat': 3,\n",
       "         'is': 2,\n",
       "         'the': 3,\n",
       "         'box': 3,\n",
       "         '.': 3,\n",
       "         'likes': 1,\n",
       "         'over': 1})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter(word_tokenize('The cat is the box. The cat likes the box. The box is over the cat.'))\n",
    "\n",
    "counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 3), ('cat', 3)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples:\n",
    "\n",
    "1. Tokenization to create a bag of words\n",
    "\n",
    "2. Lowercasing words\n",
    "\n",
    "3. Lemmatization/Stemming: shorten words to their root stems\n",
    "    \n",
    "4. Removing stop words, punctuation, or unwanted tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'cat',\n",
       " 'is',\n",
       " 'the',\n",
       " 'box',\n",
       " 'the',\n",
       " 'cat',\n",
       " 'likes',\n",
       " 'the',\n",
       " 'box',\n",
       " 'the',\n",
       " 'box',\n",
       " 'is',\n",
       " 'over',\n",
       " 'the',\n",
       " 'cat']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "text  = 'The cat is the box. The cat likes the box. The box is over the cat.'\n",
    "\n",
    "#Tokenization\n",
    "tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()] #.isalpha will return True if the string only has alphabetical characters\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'box', 'cat', 'likes', 'box', 'box', 'cat']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove stop words\n",
    "\n",
    "no_stop = [t for t in tokens if t not in stopwords.words('english')]\n",
    "\n",
    "no_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 3), ('box', 3)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a Counter\n",
    "\n",
    "counter = Counter(no_stop).most_common(2)\n",
    "\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 4,\n",
       " ',': 9,\n",
       " 'action': 10,\n",
       " 'awesome': 11,\n",
       " 'boring': 12,\n",
       " 'but': 13,\n",
       " 'characters': 14,\n",
       " 'good': 0,\n",
       " 'i': 5,\n",
       " 'is': 1,\n",
       " 'like': 6,\n",
       " 'movie': 7,\n",
       " 'movis': 2,\n",
       " 'really': 8,\n",
       " 'scenes': 15,\n",
       " 'the': 3}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_document = ['The movis is good', 'I really like the movie!','Awesome action scenes, but boring characters']\n",
    "\n",
    "tokenize_doc = [word_tokenize(doc.lower()) for doc in my_document]\n",
    "\n",
    "dictionary = Dictionary(tokenize_doc)\n",
    "\n",
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1)],\n",
       " [(3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)],\n",
       " [(9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1)]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in tokenize_doc]\n",
    "\n",
    "corpus  #Each list represent one document; for each (), the first element represents token id and the second represent the \n",
    "        #token frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defaultdict:\n",
    "\n",
    "#### allows us to initialize a dictionary that will assign a default value to non-existent keys. By supplying the argument int, we are able to ensure that any non-existent keys are automatically assigned a default value of 0.\n",
    "\n",
    "### itertools.chain.from_iterable():\n",
    "\n",
    "#### allows us to iterate through a list of list: it will flat a list of list as a whole list and iterate through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# Save the fifth document: doc\n",
    "doc = corpus[4]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "    \n",
    "# Create a sorted list from the defaultdict: sorted_word_count\n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id,word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf with gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition: \n",
    "\n",
    "#### Term-frequency - inverse document frequency\n",
    "\n",
    "1. Allows you to determin the most important words in each document\n",
    "\n",
    "2. Each corpus may have shared words beyond just stopwords and those words should be down-weighted in importance, which tf-idf can do\n",
    "\n",
    "3. Ensure most common words don't show up as key words\n",
    "\n",
    "4. keeps document specific frequent words weighted high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src = \"Tf-idf_formula.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 0.16284991207632715),\n",
       " (4, 0.44124367556640004),\n",
       " (5, 0.44124367556640004),\n",
       " (6, 0.44124367556640004),\n",
       " (7, 0.44124367556640004),\n",
       " (8, 0.44124367556640004)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "tfidf[corpus[1]] #Reach each documnet of corpus; (token_id, token_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Chapter 3. Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Def:\n",
    "\n",
    "1. Identify important named entities in the text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford CoreNLP Library\n",
    "\n",
    "1. Integrated into Python via nltk\n",
    "2. Java based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('In', 'IN'),\n",
       " ('New', 'NNP'),\n",
       " ('York', 'NNP'),\n",
       " (',', ','),\n",
       " ('I', 'PRP'),\n",
       " ('like', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('ride', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('Metro', 'NNP'),\n",
       " ('to', 'TO'),\n",
       " ('visit', 'VB'),\n",
       " ('MOMA', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('some', 'DT'),\n",
       " ('restaurants', 'NNS'),\n",
       " ('rated', 'VBN'),\n",
       " ('well', 'RB'),\n",
       " ('by', 'IN'),\n",
       " ('Ruth', 'NNP'),\n",
       " ('Reivhl', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example\n",
    "import nltk\n",
    "\n",
    "sentence = 'In New York, I like to ride the Metro to visit MOMA and some restaurants rated well by Ruth Reivhl.'\n",
    "\n",
    "tokenized_sent = word_tokenize(sentence)\n",
    "\n",
    "tagged_sent = nltk.pos_tag(tokenized_sent) #add tags to each word to specify like pronouns etc.\n",
    "\n",
    "tagged_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  In/IN\n",
      "  (GPE New/NNP York/NNP)\n",
      "  ,/,\n",
      "  I/PRP\n",
      "  like/VBP\n",
      "  to/TO\n",
      "  ride/VB\n",
      "  the/DT\n",
      "  (ORGANIZATION Metro/NNP)\n",
      "  to/TO\n",
      "  visit/VB\n",
      "  (ORGANIZATION MOMA/NNP)\n",
      "  and/CC\n",
      "  some/DT\n",
      "  restaurants/NNS\n",
      "  rated/VBN\n",
      "  well/RB\n",
      "  by/IN\n",
      "  (PERSON Ruth/NNP Reivhl/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(nltk.ne_chunk(tagged_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize the article into sentences: sentences\n",
    "sentences = sent_tokenize(article)\n",
    "\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences,binary = True)\n",
    "\n",
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)\n",
    "            \n",
    "# Create the defaultdict: ner_categories\n",
    "ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values: values\n",
    "values = [ner_categories.get(v) for v in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to SpaCy\n",
    "\n",
    "1. Similar to gensim, focusing on creating NLP pipelines to generate models and corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berlin GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "\n",
    "nlp = spacy.load('en',parse = True,tage = True, entity = True)\n",
    "\n",
    "nlp.entity\n",
    "\n",
    "doc = nlp('Berlin is the capital of Germany; and the residence of Chancellor Angela Merkel.')\n",
    "\n",
    "doc.ents\n",
    "\n",
    "print(doc.ents[0],doc.ents[0].label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilingual NER with polyglot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'polyglot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-05afb40d3337>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpolyglot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mText\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'polyglot'"
     ]
    }
   ],
   "source": [
    "from polyglot.text import Text\n",
    "\n",
    "ptext = Text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new text object using Polyglot's Text class: txt\n",
    "txt = Text(article)\n",
    "\n",
    "# Print each of the entities found\n",
    "for ent in txt.entities:\n",
    "    print(ent)\n",
    "    \n",
    "# Print the type of ent\n",
    "print(type(ent))\n",
    "\n",
    "# Create the list of tuples: entities\n",
    "entities = [(ent.tag, ' '.join(ent)) for ent in txt.entities]\n",
    "\n",
    "# Print entities\n",
    "print(entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4. Classifying fake news using supervised learning with NLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Example\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = ...\n",
    "\n",
    "y = ['Sci-Fi']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(df['plot'],y,randomstate = 53,test_size = .33)\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words = 'english') #remove stop words in English\n",
    "\n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "count_test = count_vectorizer.transform(X_test.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words = 'english',max_df = .7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A,columns = tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Print the head of count_df\n",
    "print(count_df.head())\n",
    "\n",
    "# Print the head of tfidf_df\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Calculate the difference in columns: difference\n",
    "difference = set(count_df.columns) - set(tfidf_df.columns) #Check if elements appeared in A also appeared in B\n",
    "print(difference)\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Model is commonly used for testing NLP classification problems; basis in probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  Course 2:  Adavanced NLP with Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Introduction to spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "\n",
    "#It contains the processing pipeline\n",
    "# Includes language-specific rules used for tokenizing the text into words and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "world\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "#The Doc object: Doc represents documentation\n",
    "doc = nlp('Hello world!')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "world"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = doc[1]\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world!\n"
     ]
    }
   ],
   "source": [
    "#The Span object: a slice of the document consisting of one or more tokens\n",
    "\n",
    "#Note: it's only a view of the Doc and doesn't contain any data itself\n",
    "\n",
    "\n",
    "span = doc[1:4]\n",
    "\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:   [0, 1, 2, 3, 4, 5]\n",
      "Text:    ['It', 'costs', '$', '5', '.', 'IV']\n",
      "is_alpha:   [True, True, False, False, False, True]\n",
      "is_punct:   [False, False, False, False, True, False]\n",
      "like_num:   [False, False, False, True, False, False]\n"
     ]
    }
   ],
   "source": [
    "#Lexical attributes\n",
    "\n",
    "doc = nlp(\"It costs $5. IV\")\n",
    "\n",
    "print('Index:  ', [token.i for token in doc]) # \"i\" is the index of the token within the parent document\n",
    "\n",
    "print('Text:   ', [token.text for token in doc]) #.text returns the text\n",
    "\n",
    "print('is_alpha:  ', [token.is_alpha for token in doc]) #whether the token consists of alphanumeric characters\n",
    "\n",
    "print('is_punct:  ', [token.is_punct for token in doc]) #whether it's a punctuation\n",
    "\n",
    "print('like_num:  ', [token.like_num for token in doc]) #whether it resembles a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Example\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\"In 1990, more than 60% of people in East Asia were in extreme poverty. Now less than 4% are.\")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i+1]\n",
    "        # Check if the next token's text equals '%'\n",
    "        if next_token.text == '%':\n",
    "            print('Percentage found:', token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Statistical Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the statistical models?\n",
    "\n",
    "#### It enables spaCy to predict linguistic attributes in context\n",
    "    (1) Part-of-speech tags\n",
    "    (2) Syntactic dependencies\n",
    "    (3) Named entities\n",
    "    \n",
    "#### Models are trained on large datasets of labeled example texts, and can be updated with more examples to fine-tune predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Model packages\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "(1) The package contains binary weights\n",
    "(2) Vocabulary\n",
    "(3) Meta information (language, pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Predicting part-of-speech tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON\n",
      "ate VERB\n",
      "the DET\n",
      "pizza NOUN\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#Process a text\n",
    "doc = nlp('She ate the pizza')\n",
    "\n",
    "#Iterate over the tokens\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_) #print the text and the predicted part-of-speech tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Predicting Syntactic Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj ate\n",
      "ate VERB ROOT ate\n",
      "the DET det pizza\n",
      "pizza NOUN dobj ate\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text) #.dep_: returns the predicted dependency label; \"head\" returns the \n",
    "                                                               #syntactic head token (i.e., the parent token this word is attached to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table for syntactic dependencies spaCy using\n",
    "\n",
    "     Label      Description        Example\n",
    "\n",
    "     nsubj    nominal subject       she\n",
    "     dobj     direct object         pizza\n",
    "     det      determiner (article)  the\n",
    "     \n",
    "     \n",
    "     \n",
    "#### Example:\n",
    "ate(VERB) -------(nsubj)---> She(PRON);\n",
    "\n",
    "ate(VERB) -------(dobj) ---> pizza(NOUN);\n",
    "\n",
    "the(DET) <-------(det)----> pizza(NOUN);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Predicting Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents: #.ents lets you access the named entities predicted by the model\n",
    "    print(ent.text, ent.label_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noun, proper singular'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tips: the exaplin method and help you get quick definitions of the most common tags and labels\n",
    "\n",
    "spacy.explain('GPE')\n",
    "\n",
    "spacy.explain('NNP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Rule-based Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why not just regular expression?\n",
    "\n",
    "1. The matcher will match on Doc objects, not just strings\n",
    "\n",
    "2. more flexible, you can search for texts, but also other lexical attributes\n",
    "\n",
    "3. use the model's predictions: example, find the word \"duck\" only if it's a verb not a noun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match patterns\n",
    "\n",
    "1. Match patterns are lists of dictionaries, each dictionary describes one token;\n",
    "\n",
    "2. keys are the names of token attributes, mapped to their expected values;\n",
    "\n",
    "Example:\n",
    "\n",
    "[{'ORTH': 'iPhone'}, {'ORTH': 'X'}] #match exact token texts\n",
    "\n",
    "[{'LOWER': 'iphone'}, {'LOWER': 'x'}] #match lexical attributes\n",
    "\n",
    "[{'LEMMA': 'buy'}, {'POS', 'NOUN'}] #match any token attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize the matcher with the shared vocabulary\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9528407286733565721, 1, 3)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add the pattern to the matcher\n",
    "pattern = [{'ORTH': 'iPhone'},{'ORTH':'X'}]\n",
    "\n",
    "matcher.add('IPHONE_PATTERN',None, pattern) #first arg: unique ID to identify which pattern was matched; \n",
    "                                            #second arg: optional callback\n",
    "                                            #pattern itself\n",
    "        \n",
    "#Process some text\n",
    "doc = nlp('New iPhone X release date leaked')\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "matches #(match ID, start index, end index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching lexical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 FIFA World Cup:\n"
     ]
    }
   ],
   "source": [
    "pattern = [\n",
    "            {'IS_DIGIT': True},\n",
    "            {'LOWER': 'fifa'},\n",
    "            {'LOWER': 'world'},\n",
    "            {'LOWER': 'cup'},\n",
    "            {'IS_PUNCT': True}]\n",
    "\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
    "\n",
    "matcher.add('LEXICAL_PATTERN',None,pattern)\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start,end in matches:\n",
    "    print(doc[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching other token attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern - [\n",
    "        {'LEMMA': 'love'},{'POS':'VERB'},\n",
    "        {'POS','NOUN'}\n",
    "]\n",
    "\n",
    "doc = nlp('I loved dogs but now I love cats more.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using operators and quantifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = [\n",
    "            {'LEMMA': 'buy'},\n",
    "            {'POS':'DET', 'OP': '?'} # optional: match 0 or 1 times\n",
    "            {'POS': 'NOUN'}\n",
    "]\n",
    "\n",
    "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "                               Description\n",
    "{'OP': '!'}:       Negation: match 0 times;\n",
    "\n",
    "{'OP': '?'}:       Optional: match 0 or 1 times;\n",
    "\n",
    "{'OP': '+'}:       Match 1 or more times;\n",
    "\n",
    "{'OP': '*'}:       Match 0 or more times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data Structures: Vocab, Lexemes and StringStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocab: stores data shared across multiple documents\n",
    "\n",
    "#### To save memory, spaCy encodes all strings to hash values, so if a word occurs more than once, we don't need to save it every time\n",
    "\n",
    "#### Strings are only stored once in the StringStore via nlp.vocab.strings\n",
    "\n",
    "#### Example:\n",
    "\n",
    "coffee_hash = nlp.vocab.strings['coffee']\n",
    "coffee_string = nlp.vocab.strings[coffee_hash]\n",
    "\n",
    "#### Hash ID can't be reversed - that's why we need to provide the shared vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash value:  3197928453018144401\n",
      "string value:  coffee\n",
      "hash value:  3197928453018144401\n"
     ]
    }
   ],
   "source": [
    "# Look up the string and hash in nlp.vocab.strings\n",
    "\n",
    "doc = nlp('I love coffee')\n",
    "\n",
    "print('hash value: ', nlp.vocab.strings['coffee'])\n",
    "\n",
    "print('string value: ', nlp.vocab.strings[3197928453018144401])\n",
    "\n",
    "print('hash value: ', doc.vocab.strings['coffee'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexemes: context-independent entries in the vocabulary\n",
    "\n",
    "### Lexemes don't have context-dependent part-of-speech tags, dependencies or entity labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee 3197928453018144401 True\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('I love coffee')\n",
    "\n",
    "lexeme = nlp.vocab['coffee']\n",
    "\n",
    "#pring the lexical attributes\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha) #.orth(hash id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Data Structures: Doc, Span and Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc:  Hello world!\n",
      "span:  Hello world\n",
      "span_with_label:  Hello world\n",
      "Hello world GREETING\n",
      "[('Hello world', 'GREETING')]\n"
     ]
    }
   ],
   "source": [
    "#Create an nlp object\n",
    "\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "    \n",
    "#Import the Doc class\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "#The words and spaces to create the doc from\n",
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False] #Indicating whther the word is followed by a space\n",
    "\n",
    "#Create a doc manually\n",
    "doc = Doc(nlp.vocab, words = words, spaces = spaces)\n",
    "\n",
    "print(\"doc: \", doc)\n",
    "\n",
    "#Create a span manually\n",
    "span = Span(doc, 0, 2) #The doc, start and end index\n",
    "\n",
    "print(\"span: \", span)\n",
    "\n",
    "#Create a span with a label\n",
    "span_with_label = Span(doc, 0, 2, label = 'GREETING') #usually write label names in upper case\n",
    "\n",
    "print(\"span_with_label: \", span_with_label) #or span_with_label.text\n",
    "\n",
    "#Add span to the doc.ents, which is writtable\n",
    "doc.ents = [span_with_label]\n",
    "\n",
    "print(doc.ents[0], doc.ents[0].label_)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tips:\n",
    "\n",
    "#### Doc and Span are very powerful and hold references and relationships of words and sentences\n",
    "\n",
    "#### If your application needs to output strings, make sure to convert the doc as late as possible; otherwise, you'll lose all relationships between the tokens\n",
    "\n",
    "#### To keep things consistent, try to use built-in token attributes wherever possible, for example, token.i for the token index\n",
    "\n",
    "#### Don't forget to always pass in the shared vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.3 Word vectors and semantic similatrity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### spaCy can compare two objects and predict how similar they are\n",
    "\n",
    "### Doc.similarity(), Span.similarity(), Token.similarity()\n",
    "\n",
    "### Take another object and return a smiliarity score (0 to 1)\n",
    "\n",
    "### Note: in order to use similarity, you need a larger spaCy model that has word vectors included, for example:\n",
    "1. en_core_web_md (medium model)\n",
    "2. en_core_web_lg (large model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_md'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-a18d624303b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Load a larger model with vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_md'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exists\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Path or Path-like to model data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_md'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "#Load a larger model with vectors\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "#Compare two documnets\n",
    "doc1 = nlp('I like fast food')\n",
    "doc2 = nlp('I like pizza')\n",
    "\n",
    "print(doc1.similarity(doc2))\n",
    "\n",
    "#Compare two tokens\n",
    "doc = nlp('I like pizza and pasta')\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "\n",
    "print(token1.similarity(token2))\n",
    "\n",
    "\n",
    "#Compare a documnet with a token\n",
    "doc = nlp('I like pizza')\n",
    "token = nlp('soap')[0]\n",
    "\n",
    "print(doc.similarity(token))\n",
    "\n",
    "\n",
    "#Compare a span with a document\n",
    "span = nlp('I like pizza and pasta')[2:5]\n",
    "doc = nlp('McDonalds sells burgers')\n",
    "\n",
    "print(span.similarity(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### How does spaCy predict similarity?\n",
    "\n",
    "#### 1. Similarity is determined using word vectors\n",
    "#### 2. Multi-dimimensional meaning representations of words\n",
    "#### 3. Generated using an algorithm like Word2Vec and lots of text\n",
    "#### 4. Can be added to spaCy's statistical models\n",
    "#### 5. Default: cosine similarity, but can be adjusted\n",
    "#### 6. Doc and Span vectors default to average of token vectors, that's also why you usually get more value out of shorter phrases with fewer irrelevant words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp('I have a banana')\n",
    "\n",
    "print(doc[3].vector) #300 dimensional vector of the word 'banana'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Useful for many applications: recommendation systems, flagging duplicates etc.\n",
    "### Keep in mind that there's no objective definition of \"similarity\". It always depends on the context and what application needs to be done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Combining models and rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span:  Golden Retriever\n",
      "Root token:  Retriever\n",
      "Root head token:  have\n",
      "Previous token:  a DET\n"
     ]
    }
   ],
   "source": [
    "#Example\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('DOG',None, [{'LOWER': 'golden'},{'LOWER': 'retriever'}])\n",
    "doc = nlp('I have a Golden Retriever')\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print('Matched span: ', span.text)\n",
    "    \n",
    "    #Get the span's root token and root head token\n",
    "    print('Root token: ', span.root.text)\n",
    "    print('Root head token: ', span.root.head.text)\n",
    "    \n",
    "    #Get the previous token and its POS tag\n",
    "    print('Previous token: ', doc[start-1].text, doc[start-1].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PhraseMatcher \n",
    "#### 1. like regex or keyword search, but with access to the tokens; \n",
    "#### 2. it takes Doc object as patterns; \n",
    "#### 3. more effecient than Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span:  Golden Retriever\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern  = nlp('Golden Retriever')\n",
    "\n",
    "matcher.add('DOG', None, pattern)\n",
    "\n",
    "doc = nlp('I have a Golden Retriever')\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print('Matched span: ', span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
